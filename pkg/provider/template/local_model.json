{
"chat": [
  {
    "id": "a42b25d8c10a",
    "name": "deepseek-r1:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "Deepseek-r1-1.5B 是 Deepseek 团队研发的语言模型，约15亿参数，属于Deepseek-R1模型系列中的轻量级版本，适合基础任务和低资源环境。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "1.1GB",
    "params_size": 1.5
  },
  {
    "id": "0a8c26691023",
    "name": "deepseek-r1:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "Deepseek-r1-7B 是一款 Deepseek 团队研发的模型，主要用于大模型推理场景，具有70亿参数，适用于多种应用场景。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "4.7GB",
    "params_size": 7
  },
  {
    "id": "28f8fd6cdc67",
    "name": "deepseek-r1:8b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "‌Deepseek-r1-8B‌是一款基于 Llama 架构的语言模型，拥有80亿的参数数量，经过深度蒸馏处理，使得模型在复杂的自然语言处理任务中表现出色，同时节省计算资源‌。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "4.9GB",
    "params_size": 8
  },
  {
    "id": "ea35dfe18182",
    "name": "deepseek-r1:14b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "Deepseek-r1-14B 是 Deepseek 团队研发的大规模语言模型，其参数量达到140亿。该模型在自然语言处理任务中表现出色，能够提供高质量的语言理解和生成能力。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "9.0GB",
    "params_size": 14
  },
  {
    "id": "a8b0c5157701",
    "name": "qwen2.5:0.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-0.5B 是阿里巴巴通义千问团队推出的第三代小参数模型，约5亿参数，属于轻量级模型‌，提供代码生成、文本理解等核心能力，适合轻量级自然语言任务。‌",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "398MB",
    "params_size": 0.5
  },
  {
    "id": "65ec06548149",
    "name": "qwen2.5:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-1.5B 是阿里巴巴通义千问团队推出的第三代轻量级模型，约15亿参数，在保持轻量化的同时，通过架构优化与数据扩展实现了与更大规模模型相近的推理能力。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "986MB",
    "params_size": 1.5
  },
  {
    "id": "357c53fb659c",
    "name": "qwen2.5:3b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-3B 是阿里巴巴通义千问团队推出的第三代模型，约30亿参数，覆盖中文、英文、日文等 29 种语言，在保持端侧部署优势的同时，通过架构优化实现了与 7B 参数模型相近的任务处理能力‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "1.9GB",
    "params_size": 3
  },
  {
    "id": "845dbda0ea48",
    "name": "qwen2.5:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-7B 是阿里巴巴通义千问团队推出的第三代模型，约70亿参数，在长文本处理、多语言支持和专业领域任务中展现了与更大规模模型相近的性能，是平衡算力成本与任务精度的优选方案。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "4.7GB",
    "params_size": 7
  },
  {
    "id": "7cdf5a0187d5",
    "name": "qwen2.5:14b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-14B 是阿里巴巴通义千问团队推出的第三代大模型，约147亿参数，支持多语言，在数学推理、代码生成和多语言任务中表现卓越。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "9.0GB",
    "params_size": 14
  },
  {
    "id": "0031bcf7459f",
    "name": "deepscaler:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "deepscaler-1.5B 是基于 ‌Deepseek-R1-Distilled-Qwen-1.5B 通过强化学习微调优化的语言模型，约15亿参数，适用于轻量化设备部署与实时交互场景。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "3.6GB",
    "params_size": 1.5
  },
  {
    "id": "d392ed348d5b",
    "name": "qwen2.5-coder:0.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-coder-0.5B 是阿里巴巴通义千问团队推出的编程向第三代模型，约5亿参数，基于 Qwen2.5 架构‌与 Transformer 解码器‌设计，支持 Python 等92种编程语言。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "531MB",
    "params_size": 0.5
  },
  {
    "id": "6d3abb8d2d53",
    "name": "qwen2.5-coder:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-coder-1.5B 是阿里巴巴通义千问团队推出的编程向第三代模型，约15亿参数，支持 92 种编程语言，针对轻量化设备提供高效代码生成、补全与修复能力。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "986MB",
    "params_size": 1.5
  },
  {
    "id": "e7149271c296",
    "name": "qwen2.5-coder:3b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-coder-3B 是阿里巴巴通义千问团队推出的编程向第三代模型，约30亿参数，基于 Qwen2.5 架构‌与 Transformer 解码器‌设计，重点增强多语言代码生成与优化能力，适用于中等复杂度编程任务。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "1.9GB",
    "params_size": 3
  },
  {
    "id": "2b0496514337",
    "name": "qwen2.5-coder:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-coder-7B 是阿里巴巴通义千问团队推出的编程向第三代模型，约70亿参数，支持 92 种编程语言，重点提升代码推理与修复能力，适用于复杂编程任务。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "4.7GB",
    "params_size": 7
  },
  {
    "id": "3028237cc8c5",
    "name": "qwen2.5-coder:14b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2.5-coder-14B 是阿里巴巴通义千问团队推出的编程向第三代大模型，约140亿参数，支持 92 种编程语言，重点优化大型代码库协同开发与复杂逻辑推理能力。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "9.0GB",
    "params_size": 14
  },
  {
    "id": "186c460ee707",
    "name": "yi-coder:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/yi.png",
    "description": "yi-coder-1.5B 是零一万物开源的 1.5 亿参数‌编程助手模型‌，支持52种编程语言‌‌，擅长代码生成、代码补全和调试任务‌。",
    "class": "文本生成",
    "provider": "Yi",
    "size": "866MB",
    "params_size": 1.5
  },
  {
    "id": "39c63e7675d7",
    "name": "yi-coder:9b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/yi.png",
    "description": "yi-coder-9B 是零一万物开源的90亿参数‌代码模型‌，支持52种编程语言‌‌，在代码生成、调试与补全任务中性能超越同规模模型，适用于复杂项目级开发‌。",
    "class": "文本生成",
    "provider": "Yi",
    "size": "5.0GB",
    "params_size": 9
  },
  {
    "id": "a4fdda0c6cc5",
    "name": "qwen2-math:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2-math-1.5B 是阿里巴巴通义千问团队推出的数学与推理向第二代模型，约15亿参数，适用于多步逻辑推理与竞赛数学题求解‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "935MB",
    "params_size": 1.5
  },
  {
    "id": "28cc3a337734",
    "name": "qwen2-math:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2-math-7B 是阿里巴巴通义千问团队推出的数学与推理向第二代模型，约70亿参数，擅长多步逻辑推理与竞赛数学题求解‌，支持中英双语数学问题解析‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "4.4GB",
    "params_size": 7
  },
  {
    "id": "5b699761eca5",
    "name": "glm4:9b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/glm.png",
    "description": "glm4:9B 是清华智谱开源的90亿参数‌多模态对话模型‌,支持26种语言,具备代码执行、网页浏览及多模态交互能力‌。",
    "class": "文本生成",
    "provider": "智谱",
    "size": "5.5GB",
    "params_size": 9
  },
  {
    "id": "63fb193b3a9b",
    "name": "Deepseek-coder-v2:16b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "Deepseek-coder-v2:16B 是深度求索推出的160亿参数‌代码大模型‌，支持338种编程语言‌的代码生成与理解任务‌。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "8.9GB",
    "params_size": 14
  },
  {
    "id": "6f48b936a09f",
    "name": "qwen2:0.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2-0.5B 是阿里巴巴通义千问团队推出的第二代模型，约5亿参数，面向入门级应用和小型任务开发‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "352MB",
    "params_size": 0.5
  },
  {
    "id": "f6daf2b25194",
    "name": "qwen2:1.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2-1.5B 是阿里巴巴通义千问团队推出的第二代模型，约15亿参数，擅长文本分类、数学推理等任务‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "935MB",
    "params_size": 1.5
  },
  {
    "id": "dd314f039b9d",
    "name": "qwen2:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen2-7B 是阿里巴巴通义千问团队推出的第二代模型，约70亿参数，支持 ‌27 种语言。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "4.4GB",
    "params_size": 7
  },
  {
    "id": "7c8c332f2df7",
    "name": "deepseek-v2:16b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "Deepseek-v2:16B 是 Deepseek 团队开发的升级版大语言模型，约160亿参数，擅长代码生成、数学推理等任务‌。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "8.9GB",
    "params_size": 16
  },
  {
    "id": "df352abf55b1",
    "name": "codeqwen:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "codeQwen-7B 是阿里巴巴通义千问团队推出的编程模型，约70亿参数，支持多语言编程，作为通义灵码的核心技术底座提供高效编程辅助‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "4.2GB",
    "params_size": 7
  },
  {
    "id": "b5dc5e784f2a",
    "name": "qwen:0.5b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen-0.5B 是阿里巴巴通义千问团队推出的轻量级模型，约5亿参数，专为低成本部署设计，适用于入门级AI应用场景‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "395MB",
    "params_size": 0.5
  },
  {
    "id": "b6e8ec2e7126",
    "name": "qwen:1.8b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen-1.8B 是阿里巴巴通义千问团队推出的模型，约18亿参数，具备多语言处理能力‌与‌稀疏注意力机制‌‌，适用于轻量化场景下的文本生成、问答等任务‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "1.1GB",
    "params_size": 1.8
  },
  {
    "id": "d53d04290064",
    "name": "qwen:4b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen-4B 是阿里巴巴通义千问团队推出的模型，约40亿参数，支持多语言，适用于本地化代码生成、文档分析等中等负载场景‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "2.3GB",
    "params_size": 4
  },
  {
    "id": "2091ee8c8d8f",
    "name": "qwen:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/qwen.png",
    "description": "Qwen-7B 是阿里巴巴通义千问团队推出的模型，约70亿参数，适用于文本生成、智能问答及本地化知识库构建等中高负载场景‌。",
    "class": "文本生成",
    "provider": "阿里巴巴",
    "size": "4.5GB",
    "params_size": 7
  },
  {
    "id": "9aab369a853b",
    "name": "deepseek-llm:7b",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/deepseek.png",
    "description": "Deepseek-llm-7B 是 Deepseek 团队开发的中文/多语言大模型，约70亿参数，在数学推理、代码生成等任务中表现优异‌。",
    "class": "文本生成",
    "provider": "Deepseek",
    "size": "4.0GB",
    "params_size": 7
  }
],
  "embed": [
    {
    "id": "0a109f422b47",
    "name": "nomic-embed-text",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/Nomic.png",
    "description": "nomic-embed-text是一个具有大型标记上下文窗口的高性能开放嵌入模型，也是一个基于 Sentence Transformers 库的句子嵌入模型，专门用于特征提取和句子相似度计算。在多个任务上表现出色，特别是在分类、检索和聚类任务中。",
    "class": "文本向量化",
    "provider": "Nomic",
    "size": "274MB"
  },
    {
    "id": "468836162de7",
    "name": "mxbai-embed-large",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/Mixedbread.png",
    "description": "mxbai-embed-large来自mixedbread.ai的最先进的大型嵌入模型，该模型在 MTEB 上实现了 Bert-large 规模模型的 SOTA 性能。其性能超越了 OpenAItext-embedding-3-large模型等商业模型，并与其 20 倍规模模型的性能相当,在没有 MTEB 数据重叠的情况下进行训练，这表明该模型在多个领域、任务和文本长度方面具有良好的泛化能力。",
    "class": "文本向量化",
    "provider": "Mixedbread",
    "size": "670MB"
  },
    {
    "id": "790764642607",
    "name": "bge-m3:567m",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/BAAI.png",
    "description": "BGE-M3 是 BAAI 推出的一款新机型，以多功能、多语言和多粒度的多功能性而著称。",
    "class": "文本向量化",
    "provider": "BAAI",
    "size": "1.2GB"
  },
    {
    "id": "21ab8b9b0545",
    "name": "snowflake-arctic-embed",
    "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/Snowflake.png",
    "description": "snowflake-arctic-embed是一套文本嵌入模型，专注于创建针对性能优化的高质量检索模型。\n\n这些模型利用现有的开源文本表示模型（例如 bert-base-uncased）进行训练，并在多阶段流水线中进行训练以优化其检索性能。",
    "class": "文本向量化",
    "provider": "Snowflake",
    "size": "669MB"
  },
    {
      "id": "bc8ca0995fcd651",
      "name": "quentinz/bge-large-zh-v1.5:f16",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/BAAI.png",
      "description":"bge-large可以将任何文本映射到低维密集向量，用于检索、分类、聚类或语义搜索等任务。它也可以用于 LLM 的向量数据库。",
      "class": "文本向量化",
      "provider": "BAAI",
      "size": "671MB"
    },
    {
      "id": "cd232613fa6f",
      "name": "quentinz/bge-base-zh-v1.5:f16",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/BAAI.png",
      "description":"bge-large可以将任何文本映射到低维密集向量，用于检索、分类、聚类或语义搜索等任务。它也可以用于 LLM 的向量数据库。",
      "class": "文本向量化",
      "provider": "BAAI",
      "size": "205MB"
    },
    {
      "id": "5de93a84837d",
      "name": "snowflake-arctic-embed2",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/Snowflake.png",
      "description": "Snowflake 的前沿嵌入模型。Arctic Embed 2.0 增加了多语言支持，同时又不牺牲英语的性能或可扩展性。",
      "class": "文本向量化",
      "provider": "Snowflake",
      "size": "1.2GB"
    },
    {
      "id": "1a37926bf842",
      "name": "granite-embedding:278m",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/IBM.png",
      "description": "IBM Granite Embedding 278M 模型是纯文本密集双编码器嵌入模型,适用于多语言用例。这些模型旨在为给定的文本块生成固定长度的向量表示，可用于文本相似性、检索和搜索应用。",
      "class": "文本向量化",
      "provider": "IBM",
      "size": "563MB"
    }
  ],
  "text-to-image": [
    {
      "id": "1ef250f0",
      "name": "OpenVINO/FLUX.1-schnell-fp16-ov",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/openvino_logo.png",
      "description": "",
      "class": "文生图",
      "provider": "openvino",
      "size": "34GB"

    },{
      "id": "a781cc51",
      "name": "OpenVINO/FLUX.1-schnell-int8-ov",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/openvino_logo.png",
      "description": "",
      "class": "文生图",
      "provider": "openvino",
      "size": "17GB"

    },{
      "id": "a550a9eb",
      "name": "OpenVINO/stable-diffusion-v1-5-int8-ov",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/openvino_logo.png",
      "description": "",
      "class": "文生图",
      "provider": "openvino",
      "size": "2.3GB"

    },{
      "id": "7d4b6476",
      "name": "OpenVINO/stable-diffusion-v1-5-fp16-ov",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/openvino_logo.png",
      "description": "",
      "class": "文生图",
      "provider": "openvino",
      "size": "3.1GB"

    },{
      "id": "8e898f06",
      "name": "OpenVINO/LCM_Dreamshaper_v7-fp16-ov",
      "avatar": "http://120.232.136.73:31619/byzedev/model_avatar/openvino_logo.png",
      "description": "",
      "class": "文生图",
      "provider": "openvino",
      "size": "1.3GB"
    }
  ]
}
